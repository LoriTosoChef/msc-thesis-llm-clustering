{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eddb2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models.generation import Model\n",
    "\n",
    "from config import HUGGINGFACE_TOKEN, GPT4ALL_PATH, LLAMA_7B_PATH, LLAMA_13B_PATH\n",
    "\n",
    "from helpers.data_helpers import save_to_parquet\n",
    "from helpers.generation_helpers import generation_loop\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0db74f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooldown in seconds every 50 iterations and fast cooldown every iteration\n",
    "COOLDOWN = 120\n",
    "FAST_COOLDOWN = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba148d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c602859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:11:17,747 - INFO     | __main__   | 21711 tweets in generation set.\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_parquet('data/eval_tweets_202342.parquet')\n",
    "logger.info(f'{len(tweets)} tweets in generation set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a94b9728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:11:18,222 - INFO     | __main__   | Generating from 100 tweets.\n"
     ]
    }
   ],
   "source": [
    "N_TWEETS = 100\n",
    "logger.info(f'Generating from {N_TWEETS} tweets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e693707",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tweets = tweets.iloc[:N_TWEETS].copy()\n",
    "output_tweets.drop(columns=['created_at', 'entities'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914c34f",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9670a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Answer the question based on the context below. \\\n",
    "    Context: You are a marketing and customer relationship management assistant, \\\n",
    "    your task is to classify a given tweet as either a \\\n",
    "    potential lead or not. Provide your detailed analysis of the following tweet \\\n",
    "    as a potential lead in the context of marketing and customer relationship management. \\\n",
    "    Tweet: {tweet} \\\n",
    "    Question: Is the above tweet a potential lead? Yes or No? Why?. \\\n",
    "    Answer: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4452046",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a9d46",
   "metadata": {},
   "source": [
    "### BLOOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6719cc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:11:20,687 - INFO     | models.generation | \n",
      "Initializing BLOOM model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 256\n"
     ]
    }
   ],
   "source": [
    "bloom = Model(\n",
    "    model_name='bloom',\n",
    "    hf_api=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7343505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:11:21,405 - INFO     | models.generation | Injecting Variables: ['tweet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the context below.     Context: You are a marketing and customer relationship management assistant,     your task is to classify a given tweet as either a     potential lead or not. Provide your detailed analysis of the following tweet     as a potential lead in the context of marketing and customer relationship management.     Tweet: {tweet}     Question: Is the above tweet a potential lead? Yes or No? Why?.     Answer: '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloom.init_prompt(template=PROMPT_TEMPLATE, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "274780c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:11:22,066 - INFO     | helpers.generation_helpers | Starting BLOOM generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f264546b114c298b09a3d3c7ec55b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:15:58,932 - INFO     | helpers.generation_helpers | Step: 50 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-04-04 23:15:58,945 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n",
      "2023-04-04 23:20:42,093 - INFO     | helpers.generation_helpers | Step: 100 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-04-04 23:20:42,115 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n",
      "2023-04-04 23:20:42,123 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "output_tweets = generation_loop(\n",
    "    model=bloom,\n",
    "    model_col='bloom',\n",
    "    n=N_TWEETS,\n",
    "    tweets=output_tweets,\n",
    "    fast_cool=FAST_COOLDOWN,\n",
    "    slow_cool=COOLDOWN  ,\n",
    "    out_dir='outputs',\n",
    "    out_name='200T_bloom_alpaca3b_alpaca_11b'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7b488",
   "metadata": {},
   "source": [
    "### Alpaca Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73ba04",
   "metadata": {},
   "source": [
    "#### Alpaca 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7757834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:20:42,221 - INFO     | models.generation | \n",
      "Initializing ALPACA-3B model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 256\n"
     ]
    }
   ],
   "source": [
    "alpaca_3b = Model(\n",
    "    model_name='alpaca-3b',\n",
    "    hf_repo='declare-lab/flan-alpaca-xl',\n",
    "    hf_api=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e454fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:20:42,938 - INFO     | models.generation | Injecting Variables: ['tweet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the context below.     Context: You are a marketing and customer relationship management assistant,     your task is to classify a given tweet as either a     potential lead or not. Provide your detailed analysis of the following tweet     as a potential lead in the context of marketing and customer relationship management.     Tweet: {tweet}     Question: Is the above tweet a potential lead? Yes or No? Why?.     Answer: '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_3b.init_prompt(template=PROMPT_TEMPLATE, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "942d9123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:20:42,972 - INFO     | helpers.generation_helpers | Starting ALPACA-3B generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971f5f1cb9b347cd88e703d50f28da3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:53:32,946 - INFO     | helpers.generation_helpers | Step: 50 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-04-04 23:53:32,963 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n",
      "2023-04-05 00:32:41,856 - INFO     | helpers.generation_helpers | Step: 100 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-04-05 00:32:41,876 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n",
      "2023-04-05 00:32:41,882 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "output_tweets = generation_loop(\n",
    "    model=alpaca_3b,\n",
    "    model_col='alpaca_3b',\n",
    "    n=N_TWEETS,\n",
    "    tweets=output_tweets,\n",
    "    fast_cool=FAST_COOLDOWN,\n",
    "    slow_cool=COOLDOWN,\n",
    "    out_dir='outputs',\n",
    "    out_name='200T_bloom_alpaca3b_alpaca_11b'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d709278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>bloom</th>\n",
       "      <th>alpaca_3b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1640851780128100352</td>\n",
       "      <td>$169 in the 1970s? jesus. that was 1/2 the pr...</td>\n",
       "      <td>Yes, the above tweet is a potential lead. The...</td>\n",
       "      <td>Yes, this tweet is a potential lead because it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1641906087107391488</td>\n",
       "      <td>foodwise, which runs the ferry plaza farmers m...</td>\n",
       "      <td>Yes, the tweet is a potential lead because it...</td>\n",
       "      <td>Yes, this tweet is a potential lead because it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1642529318835466247</td>\n",
       "      <td>check out this listing i just added to my #pos...</td>\n",
       "      <td>Yes, the above tweet is a potential lead. The...</td>\n",
       "      <td>Yes, this tweet is a potential lead because it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1642537505412231168</td>\n",
       "      <td>thank you maybelline #maybellinexbini #bini_f...</td>\n",
       "      <td>Yes, the above tweet is a potential lead. The...</td>\n",
       "      <td>Yes, this tweet is a potential lead because it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1642361972284092416</td>\n",
       "      <td>$lyft $uber - lyft staff pressed founders for ...</td>\n",
       "      <td>Yes, the above tweet is a potential lead. The...</td>\n",
       "      <td>Yes, this tweet is a potential lead because it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                          full_text  \\\n",
       "95  1640851780128100352   $169 in the 1970s? jesus. that was 1/2 the pr...   \n",
       "96  1641906087107391488  foodwise, which runs the ferry plaza farmers m...   \n",
       "97  1642529318835466247  check out this listing i just added to my #pos...   \n",
       "98  1642537505412231168   thank you maybelline #maybellinexbini #bini_f...   \n",
       "99  1642361972284092416  $lyft $uber - lyft staff pressed founders for ...   \n",
       "\n",
       "                                                bloom  \\\n",
       "95   Yes, the above tweet is a potential lead. The...   \n",
       "96   Yes, the tweet is a potential lead because it...   \n",
       "97   Yes, the above tweet is a potential lead. The...   \n",
       "98   Yes, the above tweet is a potential lead. The...   \n",
       "99   Yes, the above tweet is a potential lead. The...   \n",
       "\n",
       "                                            alpaca_3b  \n",
       "95  Yes, this tweet is a potential lead because it...  \n",
       "96  Yes, this tweet is a potential lead because it...  \n",
       "97  Yes, this tweet is a potential lead because it...  \n",
       "98  Yes, this tweet is a potential lead because it...  \n",
       "99  Yes, this tweet is a potential lead because it...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tweets.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3dca35d",
   "metadata": {},
   "source": [
    "#### Alpaca 11B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a56a23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 00:32:41,986 - INFO     | models.generation | \n",
      "Initializing ALPACA-11B model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 256\n"
     ]
    }
   ],
   "source": [
    "alpaca_11b = Model(\n",
    "    model_name='alpaca-11b',\n",
    "    hf_repo='declare-lab/flan-alpaca-xxl',\n",
    "    hf_api=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aad9a554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 00:32:42,670 - INFO     | models.generation | Injecting Variables: ['tweet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the context below.     Context: You are a marketing and customer relationship management assistant,     your task is to classify a given tweet as either a     potential lead or not. Provide your detailed analysis of the following tweet     as a potential lead in the context of marketing and customer relationship management.     Tweet: {tweet}     Question: Is the above tweet a potential lead? Yes or No? Why?.     Answer: '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_11b.init_prompt(template=PROMPT_TEMPLATE, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2e6235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 00:32:42,705 - INFO     | helpers.generation_helpers | Starting ALPACA-11B generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d92ec5c69ae44df819ef5a4028c6f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_tweets \u001b[39m=\u001b[39m generation_loop(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49malpaca_11b,\n\u001b[1;32m      3\u001b[0m     model_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39malpaca_11b\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     n\u001b[39m=\u001b[39;49mN_TWEETS,\n\u001b[1;32m      5\u001b[0m     tweets\u001b[39m=\u001b[39;49moutput_tweets,\n\u001b[1;32m      6\u001b[0m     fast_cool\u001b[39m=\u001b[39;49mFAST_COOLDOWN,\n\u001b[1;32m      7\u001b[0m     slow_cool\u001b[39m=\u001b[39;49mCOOLDOWN,\n\u001b[1;32m      8\u001b[0m     out_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moutputs\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     out_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m200T_bloom_alpaca3b_alpaca_11b\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/repos/msc-thesis-llm-clustering/helpers/generation_helpers.py:27\u001b[0m, in \u001b[0;36mgeneration_loop\u001b[0;34m(model, model_col, n, tweets, fast_cool, slow_cool, out_dir, out_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m outs \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n)]\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m i, tweet \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(tweets[\u001b[39m'\u001b[39m\u001b[39mfull_text\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[1;32m     26\u001b[0m     \u001b[39m# generate\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     llm, out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(inject_obj\u001b[39m=\u001b[39mtweet)\n\u001b[1;32m     28\u001b[0m     \u001b[39m# insert output\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     outs[i] \u001b[39m=\u001b[39m out\n",
      "File \u001b[0;32m~/Documents/repos/msc-thesis-llm-clustering/models/generation.py:74\u001b[0m, in \u001b[0;36mModel.generate\u001b[0;34m(self, inject_obj)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRunning Text Generation\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m     out \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mrun(inject_obj)\n\u001b[1;32m     75\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     76\u001b[0m     logger\u001b[39m.\u001b[39mwarning(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/llm.py:57\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply([inputs])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/llm.py:118\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[1;32m    117\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[1;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/llm.py:62\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(prompts, stop)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:107\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m, prompts: List[PromptValue], stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    106\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:324\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    322\u001b[0m generations \u001b[39m=\u001b[39m []\n\u001b[1;32m    323\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m--> 324\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    325\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    326\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/huggingface_hub.py:103\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to HuggingFace Hub's inference endpoint.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m        response = hf(\"Tell me a joke.\")\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m _model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m--> 103\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient(inputs\u001b[39m=\u001b[39;49mprompt, params\u001b[39m=\u001b[39;49m_model_kwargs)\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m response:\n\u001b[1;32m    105\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError raised by inference API: \u001b[39m\u001b[39m{\u001b[39;00mresponse[\u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/huggingface_hub/inference_api.py:182\u001b[0m, in \u001b[0;36mInferenceApi.__call__\u001b[0;34m(self, inputs, params, data, raw_response)\u001b[0m\n\u001b[1;32m    179\u001b[0m     payload[\u001b[39m\"\u001b[39m\u001b[39mparameters\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m params\n\u001b[1;32m    181\u001b[0m \u001b[39m# Make API call\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mpost(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_url, headers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders, json\u001b[39m=\u001b[39;49mpayload, data\u001b[39m=\u001b[39;49mdata)\n\u001b[1;32m    184\u001b[0m \u001b[39m# Let the user handle the response\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m raw_response:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_tweets = generation_loop(\n",
    "    model=alpaca_11b,\n",
    "    model_col='alpaca_11b',\n",
    "    n=N_TWEETS,\n",
    "    tweets=output_tweets,\n",
    "    fast_cool=FAST_COOLDOWN,\n",
    "    slow_cool=COOLDOWN,\n",
    "    out_dir='outputs',\n",
    "    out_name='200T_bloom_alpaca3b_alpaca_11b'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeca1e6",
   "metadata": {},
   "source": [
    "### GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "296565e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 19:26:22,097 - INFO     | models.generation | \n",
      "Initializing GPT4ALL model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 256\n",
      "llama_model_load: loading model from '.models/gpt4all-7B/gpt4all-converted.bin' - please wait ...\n",
      "llama_model_load: n_vocab = 32001\n",
      "llama_model_load: n_ctx   = 2048\n",
      "llama_model_load: n_embd  = 4096\n",
      "llama_model_load: n_mult  = 256\n",
      "llama_model_load: n_head  = 32\n",
      "llama_model_load: n_layer = 32\n",
      "llama_model_load: n_rot   = 128\n",
      "llama_model_load: f16     = 2\n",
      "llama_model_load: n_ff    = 11008\n",
      "llama_model_load: n_parts = 1\n",
      "llama_model_load: type    = 1\n",
      "llama_model_load: ggml map size = 4017.70 MB\n",
      "llama_model_load: ggml ctx size =  81.25 KB\n",
      "llama_model_load: mem required  = 5809.78 MB (+ 2052.00 MB per state)\n",
      "llama_model_load: loading tensors from '.models/gpt4all-7B/gpt4all-converted.bin'\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n"
     ]
    }
   ],
   "source": [
    "gpt4all = Model(\n",
    "    model_name='gpt4all',\n",
    "    n_threads=6,\n",
    "    local_model_path=GPT4ALL_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba7a2a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 19:26:26,005 - INFO     | models.generation | Injecting Variables: ['tweet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the context below.     Context: You are a marketing and customer relationship management assistant,     your task is to classify a given tweet as either a     potential lead or not. Provide your detailed analysis of the following tweet     as a potential lead in the context of marketing and customer relationship management.     Tweet: {tweet}     Question: Is the above tweet a potential lead? Yes or No? Why?.     Answer: '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4all.init_prompt(template=PROMPT_TEMPLATE, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15780b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 19:27:00,097 - INFO     | __main__   | Starting GPT4All generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855e382cf21044bab991a436e25f9443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 19:41:47,947 - INFO     | helpers.data_helpers | output_tweets.parquet saved.\n",
      "2023-04-04 19:57:23,399 - INFO     | helpers.data_helpers | output_tweets.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Starting GPT4All generation...')\n",
    "gpt4all_outs = [' ' for _ in range(N_TWEETS)]\n",
    "for i, tweet in enumerate(tqdm(output_tweets['full_text'])):\n",
    "    gpt4all_llm, gpt4all_out = gpt4all.generate(tweet)\n",
    "    gpt4all_outs[i] = gpt4all_out\n",
    "    time.sleep(FAST_COOLDOWN)\n",
    "    if (i+1) % 50 == 0:\n",
    "        logger.info(f'{i+1} - Saving checkpoint...')\n",
    "        output_tweets['gpt4all_out'] = np.array(gpt4all_outs)\n",
    "        save_to_parquet(data_dir='outputs', df=output_tweets, name='output_tweets')\n",
    "        time.sleep(COOLDOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1edf1bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 19:59:23,482 - INFO     | helpers.data_helpers | output_tweets.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "output_tweets['gpt4all_out'] = np.array(gpt4all_outs)\n",
    "save_to_parquet(data_dir='outputs', df=output_tweets, name='output_tweets')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f821cc5c",
   "metadata": {},
   "source": [
    "### Llama 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39f1e246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 20:15:28,928 - INFO     | models.generation | \n",
      "Initializing LLAMA model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 256\n",
      "llama_model_load: loading model from '.models/llama-7B/ggml-model-q4_0.bin' - please wait ...\n",
      "llama_model_load: n_vocab = 32000\n",
      "llama_model_load: n_ctx   = 2048\n",
      "llama_model_load: n_embd  = 4096\n",
      "llama_model_load: n_mult  = 256\n",
      "llama_model_load: n_head  = 32\n",
      "llama_model_load: n_layer = 32\n",
      "llama_model_load: n_rot   = 128\n",
      "llama_model_load: f16     = 2\n",
      "llama_model_load: n_ff    = 11008\n",
      "llama_model_load: n_parts = 1\n",
      "llama_model_load: type    = 1\n",
      "llama_model_load: ggml map size = 4017.70 MB\n",
      "llama_model_load: ggml ctx size =  81.25 KB\n",
      "llama_model_load: mem required  = 5809.78 MB (+ 2052.00 MB per state)\n",
      "llama_model_load: loading tensors from '.models/llama-7B/ggml-model-q4_0.bin'\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n"
     ]
    }
   ],
   "source": [
    "llama_7b = Model(\n",
    "    model_name='llama',\n",
    "    n_threads=6,\n",
    "    local_model_path=LLAMA_7B_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f14c7b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 20:15:33,957 - INFO     | models.generation | Injecting Variables: ['tweet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the context below.     Context: You are a marketing and customer relationship management assistant,     your task is to classify a given tweet as either a     potential lead or not. Provide your detailed analysis of the following tweet     as a potential lead in the context of marketing and customer relationship management.     Tweet: {tweet}     Question: Is the above tweet a potential lead? Yes or No? Why?.     Answer: '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_7b.init_prompt(template=PROMPT_TEMPLATE, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efdc035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 20:15:42,038 - INFO     | __main__   | Starting Llama 7B generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d23ab6ed754ae4bf59a1dd67f70566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 20:43:13,640 - INFO     | __main__   | 50 - Saving checkpoint...\n",
      "2023-04-04 20:43:13,653 - INFO     | helpers.data_helpers | output_tweets.parquet saved.\n",
      "2023-04-04 21:12:21,825 - INFO     | __main__   | 100 - Saving checkpoint...\n",
      "2023-04-04 21:12:21,832 - INFO     | helpers.data_helpers | output_tweets.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Starting Llama 7B generation...')\n",
    "llama_7b_outs = [' ' for _ in range(N_TWEETS)]\n",
    "for i, tweet in enumerate(tqdm(output_tweets['full_text'])):\n",
    "    llama_7b_llm, llama_7b_out = llama_7b.generate(tweet)\n",
    "    llama_7b_outs[i] = llama_7b_out\n",
    "    time.sleep(FAST_COOLDOWN)\n",
    "    if (i+1) % 50 == 0:\n",
    "        logger.info(f'{i+1} - Saving checkpoint...')\n",
    "        output_tweets['llama_7b_out'] = np.array(llama_7b_outs)\n",
    "        save_to_parquet(data_dir='outputs', df=output_tweets, name='output_tweets')\n",
    "        time.sleep(COOLDOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98b7ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 21:14:21,915 - INFO     | helpers.data_helpers | output_tweets.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "output_tweets['llama_7b_out'] = np.array(llama_7b_outs)\n",
    "save_to_parquet(data_dir='outputs', df=output_tweets, name='output_tweets')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3f78f28",
   "metadata": {},
   "source": [
    "### Llama 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49590f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_13b = Model(\n",
    "    model_name='llama',\n",
    "    n_threads=6,\n",
    "    local_model_path=LLAMA_13B_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67639272",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_13b.init_prompt(template=PROMPT_TEMPLATE, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb549e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Starting Llama 13B generation...')\n",
    "llama_13b_outs = [' ' for _ in range(N_TWEETS)]\n",
    "for i, tweet in enumerate(tqdm(output_tweets['full_text'])):\n",
    "    llama_13b_llm, llama_13b_out = llama_13b.generate(tweet)\n",
    "    llama_13b_outs[i] = llama_13b_out\n",
    "    time.sleep(FAST_COOLDOWN)\n",
    "    if (i+1) % 50 == 0:\n",
    "        logger.info(f'{i+1} - Saving checkpoint...')\n",
    "        output_tweets['llama_13b_out'] = np.array(llama_13b_outs)\n",
    "        save_to_parquet(data_dir='outputs', df=output_tweets, name='output_tweets')\n",
    "        time.sleep(COOLDOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tweets['llama_13b_out'] = np.array(llama_13b_outs)\n",
    "save_to_parquet(data_dir='outputs', df=output_tweets, name='output_tweets')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
