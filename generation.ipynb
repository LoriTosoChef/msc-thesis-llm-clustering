{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eddb2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models.generation import Model\n",
    "\n",
    "from config import HUGGINGFACE_TOKEN, GPT4ALL_PATH, LLAMA_7B_PATH, LLAMA_13B_PATH, OPENAI_API_KEY\n",
    "\n",
    "from helpers.data_helpers import save_to_parquet\n",
    "from helpers.generation_helpers import generation_loop\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db74f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooldown in seconds every 50 iterations and fast cooldown every iteration\n",
    "SLOW_COOLDOWN = 120\n",
    "FAST_COOLDOWN = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72ba148d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c602859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 18:26:05,226 - INFO     | __main__   | 21711 tweets in generation set.\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_parquet('data/eval_tweets_202342.parquet')\n",
    "logger.info(f'{len(tweets)} tweets in generation set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94b9728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 18:26:05,721 - INFO     | __main__   | Generating from 50 tweets.\n"
     ]
    }
   ],
   "source": [
    "N_TWEETS = 50\n",
    "logger.info(f'Generating from {N_TWEETS} tweets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e693707",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tweets = tweets.iloc[:N_TWEETS].copy()\n",
    "output_tweets.drop(columns=['created_at', 'entities'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b914c34f",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9670a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Answer the question based on the context below. \\\n",
    "    Context: You are a marketing and customer relationship management assistant, \\\n",
    "    your task is to classify a given tweet as either a \\\n",
    "    potential lead or not. Provide your detailed analysis of the following tweet \\\n",
    "    as a potential lead in the context of marketing and customer relationship management. \\\n",
    "    Tweet: \"{tweet}\" \\\n",
    "    Question: Is the above tweet a potential lead? Yes or No? Why?. \\\n",
    "    Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ff970a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE_2 = \"\"\"You are an expert marketing and customer relationship management assistant. \n",
    "Your task is, given a tweet taken from social media, to classify it as either a potential lead or not, \n",
    "from a marketing point of view. Answer in 100 words or less. Below are two examples. DO NOT always use the same answer as provided in the examples.\n",
    "\n",
    "Tweet: \"Bad relationships dont just hurt you emotionally. They also have physical effects on your body.\"\n",
    "Answer: No, this tweet is not a potential lead as it does not contain any reference to any products or services.\n",
    "\n",
    "Tweet: \"If I want to get into reading managa / comics on iPad, whats the equivalent to kindle / Netflix for that?\"\n",
    "Answer: Yesm this tweet could be a potential lead as it mentions different products and mentions an intent to a purchase.\n",
    "\n",
    "Now answer the question, is the following tweet a potential lead? Yes or No? Why?.\n",
    "Tweet: \"{tweet}\".\n",
    "Answer: \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4452046",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "668a9d46",
   "metadata": {},
   "source": [
    "### BLOOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6719cc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:11:20,687 - INFO     | models.generation | \n",
      "Initializing BLOOM model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 256\n"
     ]
    }
   ],
   "source": [
    "bloom = Model(\n",
    "    model_name='bloom',\n",
    "    hf_api=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7343505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:11:21,405 - INFO     | models.generation | Injecting Variables: ['tweet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the context below.     Context: You are a marketing and customer relationship management assistant,     your task is to classify a given tweet as either a     potential lead or not. Provide your detailed analysis of the following tweet     as a potential lead in the context of marketing and customer relationship management.     Tweet: {tweet}     Question: Is the above tweet a potential lead? Yes or No? Why?.     Answer: '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloom.init_prompt(template=PROMPT_TEMPLATE, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "274780c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:11:22,066 - INFO     | helpers.generation_helpers | Starting BLOOM generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f264546b114c298b09a3d3c7ec55b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:15:58,932 - INFO     | helpers.generation_helpers | Step: 50 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-04-04 23:15:58,945 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n",
      "2023-04-04 23:20:42,093 - INFO     | helpers.generation_helpers | Step: 100 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-04-04 23:20:42,115 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n",
      "2023-04-04 23:20:42,123 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "output_tweets = generation_loop(\n",
    "    model=bloom,\n",
    "    model_col='bloom',\n",
    "    n=N_TWEETS,\n",
    "    tweets=output_tweets,\n",
    "    fast_cool=FAST_COOLDOWN,\n",
    "    slow_cool=SLOW_COOLDOWN  ,\n",
    "    out_dir='outputs',\n",
    "    out_name='200T_bloom_alpaca3b_alpaca_11b'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bd7b488",
   "metadata": {},
   "source": [
    "### Alpaca Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ff6646a",
   "metadata": {},
   "source": [
    "#### Alpaca 770M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2561271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 18:43:54,070 - INFO     | models.generation | \n",
      "Initializing ALPACA-770M model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 256\n"
     ]
    }
   ],
   "source": [
    "alpaca_770m = Model(\n",
    "    model_name='alpaca-770m',\n",
    "    hf_repo='declare-lab/flan-alpaca-large',\n",
    "    hf_api=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d95a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert marketing and customer relationship management assistant. \\nYour task is, given a tweet taken from social media, to classify it as either a potential lead or not, \\nfrom a marketing point of view. Answer in 100 words or less. Below are two examples. DO NOT always use the same answer as provided in the examples.\\n\\nTweet: \"Bad relationships dont just hurt you emotionally. They also have physical effects on your body.\"\\nAnswer: No, this tweet is not a potential lead as it does not contain any reference to any products or services.\\n\\nTweet: \"If I want to get into reading managa / comics on iPad, whats the equivalent to kindle / Netflix for that?\"\\nAnswer: Yesm this tweet could be a potential lead as it mentions different products and mentions an intent to a purchase.\\n\\nNow answer the question, is the following tweet a potential lead? Yes or No? Why?.\\nTweet: \"{tweet}\".\\nAnswer: '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_770m.init_prompt(template=PROMPT_TEMPLATE_2, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f213d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 18:43:54,697 - INFO     | helpers.generation_helpers | Starting ALPACA-770M generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd528df622d541d0b92832ac536bd269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 18:51:43,991 - INFO     | helpers.generation_helpers | Step: 50 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-07-08 18:53:44,015 - INFO     | helpers.data_helpers | test_prompt_2.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "output_tweets = generation_loop(\n",
    "    model=alpaca_770m,\n",
    "    model_col='alpaca_770m',\n",
    "    n=N_TWEETS,\n",
    "    tweets=output_tweets,\n",
    "    fast_cool=FAST_COOLDOWN,\n",
    "    slow_cool=SLOW_COOLDOWN,\n",
    "    out_dir='outputs',\n",
    "    out_name='test_prompt_2'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a73ba04",
   "metadata": {},
   "source": [
    "#### Alpaca 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7757834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:20:42,221 - INFO     | models.generation | \n",
      "Initializing ALPACA-3B model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 256\n"
     ]
    }
   ],
   "source": [
    "alpaca_3b = Model(\n",
    "    model_name='alpaca-3b',\n",
    "    hf_repo='declare-lab/flan-alpaca-xl',\n",
    "    hf_api=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e454fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:20:42,938 - INFO     | models.generation | Injecting Variables: ['tweet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the context below.     Context: You are a marketing and customer relationship management assistant,     your task is to classify a given tweet as either a     potential lead or not. Provide your detailed analysis of the following tweet     as a potential lead in the context of marketing and customer relationship management.     Tweet: {tweet}     Question: Is the above tweet a potential lead? Yes or No? Why?.     Answer: '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_3b.init_prompt(template=PROMPT_TEMPLATE, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "942d9123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:20:42,972 - INFO     | helpers.generation_helpers | Starting ALPACA-3B generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971f5f1cb9b347cd88e703d50f28da3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:53:32,946 - INFO     | helpers.generation_helpers | Step: 50 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-04-04 23:53:32,963 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n",
      "2023-04-05 00:32:41,856 - INFO     | helpers.generation_helpers | Step: 100 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-04-05 00:32:41,876 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n",
      "2023-04-05 00:32:41,882 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "output_tweets = generation_loop(\n",
    "    model=alpaca_3b,\n",
    "    model_col='alpaca_3b',\n",
    "    n=N_TWEETS,\n",
    "    tweets=output_tweets,\n",
    "    fast_cool=FAST_COOLDOWN,\n",
    "    slow_cool=SLOW_COOLDOWN,\n",
    "    out_dir='outputs',\n",
    "    out_name='200T_bloom_alpaca3b_alpaca_11b'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdeca1e6",
   "metadata": {},
   "source": [
    "### GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "296565e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 18:54:22,532 - INFO     | models.generation | \n",
      "Initializing GPT4ALL model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 256\n",
      "llama_model_load: loading model from '.models/gpt4all-7B/gpt4all-converted.bin' - please wait ...\n",
      "llama_model_load: n_vocab = 32001\n",
      "llama_model_load: n_ctx   = 2048\n",
      "llama_model_load: n_embd  = 4096\n",
      "llama_model_load: n_mult  = 256\n",
      "llama_model_load: n_head  = 32\n",
      "llama_model_load: n_layer = 32\n",
      "llama_model_load: n_rot   = 128\n",
      "llama_model_load: f16     = 2\n",
      "llama_model_load: n_ff    = 11008\n",
      "llama_model_load: n_parts = 1\n",
      "llama_model_load: type    = 1\n",
      "llama_model_load: ggml map size = 4017.70 MB\n",
      "llama_model_load: ggml ctx size =  81.25 KB\n",
      "llama_model_load: mem required  = 5809.78 MB (+ 2052.00 MB per state)\n",
      "llama_model_load: loading tensors from '.models/gpt4all-7B/gpt4all-converted.bin'\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n"
     ]
    }
   ],
   "source": [
    "gpt4all = Model(\n",
    "    model_name='gpt4all',\n",
    "    n_threads=6,\n",
    "    local_model_path=GPT4ALL_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba7a2a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert marketing and customer relationship management assistant. \\nYour task is, given a tweet taken from social media, to classify it as either a potential lead or not, \\nfrom a marketing point of view. Answer in 100 words or less. Below are two examples. DO NOT always use the same answer as provided in the examples.\\n\\nTweet: \"Bad relationships dont just hurt you emotionally. They also have physical effects on your body.\"\\nAnswer: No, this tweet is not a potential lead as it does not contain any reference to any products or services.\\n\\nTweet: \"If I want to get into reading managa / comics on iPad, whats the equivalent to kindle / Netflix for that?\"\\nAnswer: Yesm this tweet could be a potential lead as it mentions different products and mentions an intent to a purchase.\\n\\nNow answer the question, is the following tweet a potential lead? Yes or No? Why?.\\nTweet: \"{tweet}\".\\nAnswer: '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4all.init_prompt(template=PROMPT_TEMPLATE_2, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15780b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 18:54:35,882 - INFO     | helpers.generation_helpers | Starting GPT4ALL generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f457edcd07114e79b02d0c02b817b2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_tweets \u001b[39m=\u001b[39m generation_loop(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49mgpt4all,\n\u001b[1;32m      3\u001b[0m     model_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpt4all\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     n\u001b[39m=\u001b[39;49mN_TWEETS,\n\u001b[1;32m      5\u001b[0m     tweets\u001b[39m=\u001b[39;49moutput_tweets,\n\u001b[1;32m      6\u001b[0m     fast_cool\u001b[39m=\u001b[39;49mFAST_COOLDOWN,\n\u001b[1;32m      7\u001b[0m     slow_cool\u001b[39m=\u001b[39;49mSLOW_COOLDOWN,\n\u001b[1;32m      8\u001b[0m     out_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moutputs\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     out_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtest_prompt_2\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/repos/msc-thesis-llm-clustering/helpers/generation_helpers.py:27\u001b[0m, in \u001b[0;36mgeneration_loop\u001b[0;34m(model, model_col, n, tweets, fast_cool, slow_cool, out_dir, out_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m outs \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n)]\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m i, tweet \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(tweets[\u001b[39m'\u001b[39m\u001b[39mfull_text\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[1;32m     26\u001b[0m     \u001b[39m# generate\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     llm, out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inject_obj\u001b[39m=\u001b[39;49mtweet)\n\u001b[1;32m     28\u001b[0m     \u001b[39m# insert output\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     outs[i] \u001b[39m=\u001b[39m out\n",
      "File \u001b[0;32m~/Documents/repos/msc-thesis-llm-clustering/models/generation.py:89\u001b[0m, in \u001b[0;36mModel.generate\u001b[0;34m(self, inject_obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRunning Text Generation\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m     out \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mrun(inject_obj)\n\u001b[1;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     91\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRATE LIMIT! Waiting for 5 minutes\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/llm.py:57\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply([inputs])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/llm.py:118\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[1;32m    117\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[1;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/llm.py:62\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(prompts, stop)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:107\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m, prompts: List[PromptValue], stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    106\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:324\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    322\u001b[0m generations \u001b[39m=\u001b[39m []\n\u001b[1;32m    323\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m--> 324\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    325\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    326\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/llamacpp.py:173\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m    170\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mstop_sequences\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[1;32m    172\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call the Llama model and return the output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient(\n\u001b[1;32m    174\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m    175\u001b[0m     max_tokens\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m     temperature\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    177\u001b[0m     top_p\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    178\u001b[0m     logprobs\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    179\u001b[0m     echo\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mecho\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    180\u001b[0m     stop\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mstop_sequences\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    181\u001b[0m     repeat_penalty\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mrepeat_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    182\u001b[0m     top_k\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mtop_k\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    183\u001b[0m )\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m text[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/llama_cpp/llama.py:506\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, repeat_penalty, top_k, stream)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    471\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    472\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m     stream: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    483\u001b[0m ):\n\u001b[1;32m    484\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_completion(\n\u001b[1;32m    507\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m    508\u001b[0m         suffix\u001b[39m=\u001b[39;49msuffix,\n\u001b[1;32m    509\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m    510\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    511\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m    512\u001b[0m         logprobs\u001b[39m=\u001b[39;49mlogprobs,\n\u001b[1;32m    513\u001b[0m         echo\u001b[39m=\u001b[39;49mecho,\n\u001b[1;32m    514\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    515\u001b[0m         repeat_penalty\u001b[39m=\u001b[39;49mrepeat_penalty,\n\u001b[1;32m    516\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m    517\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    518\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/llama_cpp/llama.py:467\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, repeat_penalty, top_k, stream)\u001b[0m\n\u001b[1;32m    465\u001b[0m     chunks: Iterator[CompletionChunk] \u001b[39m=\u001b[39m completion_or_chunks\n\u001b[1;32m    466\u001b[0m     \u001b[39mreturn\u001b[39;00m chunks\n\u001b[0;32m--> 467\u001b[0m completion: Completion \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(completion_or_chunks)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/llama_cpp/llama.py:310\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, repeat_penalty, top_k, stream)\u001b[0m\n\u001b[1;32m    307\u001b[0m     stop_sequences \u001b[39m=\u001b[39m []\n\u001b[1;32m    309\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    311\u001b[0m     prompt_tokens,\n\u001b[1;32m    312\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    313\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    314\u001b[0m     temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    315\u001b[0m     repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    316\u001b[0m ):\n\u001b[1;32m    317\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39m==\u001b[39m llama_cpp\u001b[39m.\u001b[39mllama_token_eos():\n\u001b[1;32m    318\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/llama_cpp/llama.py:220\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    219\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    221\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    222\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    223\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    224\u001b[0m         temp\u001b[39m=\u001b[39mtemp,\n\u001b[1;32m    225\u001b[0m         repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m     tokens_or_none \u001b[39m=\u001b[39m \u001b[39myield\u001b[39;00m token\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/llama_cpp/llama.py:141\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    139\u001b[0m batch \u001b[39m=\u001b[39m tokens[i : \u001b[39mmin\u001b[39m(\u001b[39mlen\u001b[39m(tokens), i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_batch)]\n\u001b[1;32m    140\u001b[0m n_past \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_ctx \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(batch), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokens_consumed)\n\u001b[0;32m--> 141\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_eval(\n\u001b[1;32m    142\u001b[0m     ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    143\u001b[0m     tokens\u001b[39m=\u001b[39;49m(llama_cpp\u001b[39m.\u001b[39;49mllama_token \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(batch))(\u001b[39m*\u001b[39;49mbatch),\n\u001b[1;32m    144\u001b[0m     n_tokens\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mlen\u001b[39;49m(batch)),\n\u001b[1;32m    145\u001b[0m     n_past\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(n_past),\n\u001b[1;32m    146\u001b[0m     n_threads\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_threads),\n\u001b[1;32m    147\u001b[0m )\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mint\u001b[39m(return_code) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    149\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_eval returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/llama_cpp/llama_cpp.py:178\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama_eval\u001b[39m(\n\u001b[1;32m    172\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m    173\u001b[0m     tokens,  \u001b[39m# type: Array[llama_token]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m     n_threads: c_int,\n\u001b[1;32m    177\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m c_int:\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m _lib\u001b[39m.\u001b[39;49mllama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_tweets = generation_loop(\n",
    "    model=gpt4all,\n",
    "    model_col='gpt4all',\n",
    "    n=N_TWEETS,\n",
    "    tweets=output_tweets,\n",
    "    fast_cool=FAST_COOLDOWN,\n",
    "    slow_cool=SLOW_COOLDOWN,\n",
    "    out_dir='outputs',\n",
    "    out_name='test_prompt_2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef693a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 18:36:33,102 - INFO     | models.generation | \n",
      "Initializing OPENAI model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 500\n",
      "2023-07-08 18:36:33,114 - INFO     | helpers.generation_helpers | Starting OPENAI generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72556720672542a6b7f69760d8469ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 18:41:53,991 - INFO     | helpers.generation_helpers | Step: 50 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-07-08 18:43:54,009 - INFO     | helpers.data_helpers | test_prompt_2.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI Model\n",
    "openai = Model(model_name='openai',\n",
    "               openai_api=OPENAI_API_KEY,\n",
    "               openai_model='gpt-3.5-turbo',\n",
    "               max_tokens=500)\n",
    "\n",
    "\n",
    "# injecting prompts\n",
    "openai.init_prompt(template=PROMPT_TEMPLATE_2,\n",
    "                   input_vars=['tweet'])\n",
    "\n",
    "# generation\n",
    "output_tweets = generation_loop(model=openai,\n",
    "                                model_col='gpt-3.5-turbo',\n",
    "                                n=N_TWEETS,\n",
    "                                tweets=output_tweets,\n",
    "                                fast_cool=FAST_COOLDOWN,\n",
    "                                slow_cool=120,\n",
    "                                out_dir='outputs',\n",
    "                                out_name='test_prompt_2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f821cc5c",
   "metadata": {},
   "source": [
    "### Llama 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39f1e246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 20:15:28,928 - INFO     | models.generation | \n",
      "Initializing LLAMA model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 256\n",
      "llama_model_load: loading model from '.models/llama-7B/ggml-model-q4_0.bin' - please wait ...\n",
      "llama_model_load: n_vocab = 32000\n",
      "llama_model_load: n_ctx   = 2048\n",
      "llama_model_load: n_embd  = 4096\n",
      "llama_model_load: n_mult  = 256\n",
      "llama_model_load: n_head  = 32\n",
      "llama_model_load: n_layer = 32\n",
      "llama_model_load: n_rot   = 128\n",
      "llama_model_load: f16     = 2\n",
      "llama_model_load: n_ff    = 11008\n",
      "llama_model_load: n_parts = 1\n",
      "llama_model_load: type    = 1\n",
      "llama_model_load: ggml map size = 4017.70 MB\n",
      "llama_model_load: ggml ctx size =  81.25 KB\n",
      "llama_model_load: mem required  = 5809.78 MB (+ 2052.00 MB per state)\n",
      "llama_model_load: loading tensors from '.models/llama-7B/ggml-model-q4_0.bin'\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n"
     ]
    }
   ],
   "source": [
    "llama_7b = Model(\n",
    "    model_name='llama',\n",
    "    n_threads=6,\n",
    "    local_model_path=LLAMA_7B_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f14c7b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 20:15:33,957 - INFO     | models.generation | Injecting Variables: ['tweet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the context below.     Context: You are a marketing and customer relationship management assistant,     your task is to classify a given tweet as either a     potential lead or not. Provide your detailed analysis of the following tweet     as a potential lead in the context of marketing and customer relationship management.     Tweet: {tweet}     Question: Is the above tweet a potential lead? Yes or No? Why?.     Answer: '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_7b.init_prompt(template=PROMPT_TEMPLATE, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efdc035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 20:15:42,038 - INFO     | __main__   | Starting Llama 7B generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d23ab6ed754ae4bf59a1dd67f70566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 20:43:13,640 - INFO     | __main__   | 50 - Saving checkpoint...\n",
      "2023-04-04 20:43:13,653 - INFO     | helpers.data_helpers | output_tweets.parquet saved.\n",
      "2023-04-04 21:12:21,825 - INFO     | __main__   | 100 - Saving checkpoint...\n",
      "2023-04-04 21:12:21,832 - INFO     | helpers.data_helpers | output_tweets.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "output_tweets = generation_loop(\n",
    "    model=llama_7b,\n",
    "    model_col='llama_7b',\n",
    "    n=N_TWEETS,\n",
    "    tweets=output_tweets,\n",
    "    fast_cool=FAST_COOLDOWN,\n",
    "    slow_cool=SLOW_COOLDOWN,\n",
    "    out_dir='outputs',\n",
    "    out_name='200T_bloom_alpaca3b_alpaca_11b'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3f78f28",
   "metadata": {},
   "source": [
    "### Llama 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49590f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 10:53:09,501 - INFO     | models.generation | \n",
      "Initializing LLAMA_13B model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 256\n",
      "llama_model_load: loading model from '.models/llama-13B/ggml-model-q4_0.bin' - please wait ...\n",
      "llama_model_load: n_vocab = 32000\n",
      "llama_model_load: n_ctx   = 2048\n",
      "llama_model_load: n_embd  = 5120\n",
      "llama_model_load: n_mult  = 256\n",
      "llama_model_load: n_head  = 40\n",
      "llama_model_load: n_layer = 40\n",
      "llama_model_load: n_rot   = 128\n",
      "llama_model_load: f16     = 2\n",
      "llama_model_load: n_ff    = 13824\n",
      "llama_model_load: n_parts = 2\n",
      "llama_model_load: type    = 2\n",
      "llama_model_load: ggml map size = 7759.83 MB\n",
      "llama_model_load: ggml ctx size = 101.25 KB\n",
      "llama_model_load: mem required  = 9807.93 MB (+ 3216.00 MB per state)\n",
      "llama_model_load: loading tensors from '.models/llama-13B/ggml-model-q4_0.bin'\n",
      "llama_model_load: model size =  7759.39 MB / num tensors = 363\n",
      "llama_init_from_file: kv self size  = 3200.00 MB\n"
     ]
    }
   ],
   "source": [
    "llama_13b = Model(\n",
    "    model_name='llama_13b',\n",
    "    n_threads=6,\n",
    "    local_model_path=LLAMA_13B_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67639272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 10:53:11,087 - INFO     | models.generation | Injecting Variables: ['tweet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the context below.     Context: You are a marketing and customer relationship management assistant,     your task is to classify a given tweet as either a     potential lead or not. Provide your detailed analysis of the following tweet     as a potential lead in the context of marketing and customer relationship management.     Tweet: {tweet}     Question: Is the above tweet a potential lead? Yes or No? Why?.     Answer: '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_13b.init_prompt(template=PROMPT_TEMPLATE, input_vars=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cb549e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 10:53:24,667 - INFO     | helpers.generation_helpers | Starting LLAMA_13B generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fd0b6becda48118fea1c8f11bc5e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 11:39:09,689 - INFO     | helpers.generation_helpers | Step: 50 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-04-05 11:39:09,705 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n",
      "2023-04-05 12:24:19,503 - INFO     | helpers.generation_helpers | Step: 100 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-04-05 12:24:19,533 - INFO     | helpers.data_helpers | 200T_bloom_alpaca3b_alpaca_11b.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "output_tweets = generation_loop(\n",
    "    model=llama_13b,\n",
    "    model_col='llama_13b',\n",
    "    n=N_TWEETS,\n",
    "    tweets=output_tweets,\n",
    "    fast_cool=FAST_COOLDOWN,\n",
    "    slow_cool=SLOW_COOLDOWN,\n",
    "    out_dir='outputs',\n",
    "    out_name='200T_bloom_alpaca3b_alpaca_11b'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
