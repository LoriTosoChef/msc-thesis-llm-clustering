{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eddb2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 15:28:06,850 - INFO     | config     | Loading environment variables\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models.bloom import Bloom\n",
    "#from models.gpt4all import GPT4ALL\n",
    "\n",
    "from config import HUGGINGFACE_TOKEN\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7880c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba148d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c602859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 15:28:12,100 - INFO     | __main__   | 21711 tweets in generation set.\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_parquet('data/eval_tweets_202342.parquet')\n",
    "logger.info(f'{len(tweets)} tweets in generation set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a94b9728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 15:28:17,746 - INFO     | __main__   | Generating from 5 tweets.\n"
     ]
    }
   ],
   "source": [
    "N_TWEETS = 5\n",
    "logger.info(f'Generating from {N_TWEETS} tweets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e693707",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tweets = tweets.iloc[:N_TWEETS].copy()\n",
    "output_tweets.drop(columns=['created_at', 'entities'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914c34f",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9670a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Answer the question based on the context below. \\\n",
    "    Context: You are a marketing and customer relationship management assistant, \\\n",
    "    your task is to classify a given tweet as either a \\\n",
    "    potential lead or not. Provide your analysis of the following social media post (tweet) \\\n",
    "    as a potential lead in the context of marketing and customer relationship management. \\\n",
    "    Consider the following factors in your analysis, but feel free to use additional \\\n",
    "    factors as well: \\\n",
    "    - Mentions of product or service offerings, calls to action, inquiries about pricing \\\n",
    "    or availability \\\n",
    "    - Keywords or phrases commonly associated with potential leads in the context of \\\n",
    "    marketing and CRM \\\n",
    "    - The tone and sentiment of the tweet \\\n",
    "    - The author's profile and engagement history on social media \\\n",
    "    - Any relevant contextual factors, such as recent product launches, industry events or \\\n",
    "    trends, or competitor activity. \\\n",
    "    Tweet: {tweet} \\\n",
    "    Question: Is the above tweet a potential lead? Yes or No? Why?. \\\n",
    "    Answer: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4452046",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a9d46",
   "metadata": {},
   "source": [
    "### BLOOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6719cc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 15:28:25,351 - INFO     | models.bloom | \n",
      "Initializing BLOOM model - Temp: 1e-10 - Context window: 2048 - Max Length: 256\n",
      "2023-04-04 15:28:26,269 - INFO     | models.bloom | Injecting Variables: ['tweet']\n",
      "2023-04-04 15:28:26,270 - INFO     | models.bloom | Initializing tokenizer\n",
      "2023-04-04 15:28:28,700 - INFO     | models.bloom | Prompt tokens len: 252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloom = Bloom(hf_api=HUGGINGFACE_TOKEN, temp=1e-10, max_length=256)\n",
    "bloom.init_prompt(template=PROMPT_TEMPLATE, input_vars=['tweet'])\n",
    "bloom.count_prompt_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274780c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 15:31:57,977 - INFO     | __main__   | Starting BLOOM generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1966f273c3489896d246f3a819205d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info('Starting BLOOM generation...')\n",
    "bloom_outs = []\n",
    "for i, tweet in enumerate(tqdm(output_tweets['full_text'])):\n",
    "    llm, bloom_out = bloom.generate(inject_obj=tweet)\n",
    "    bloom_outs.append(bloom_out)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f57cad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tweets['bloom_out'] = np.array(bloom_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeca1e6",
   "metadata": {},
   "source": [
    "### GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296565e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 19:14:50,926 - INFO     | models.gpt4all | \n",
      "Initializing GPT4All model - Temp: 1e-10 - Context window: 512 - Threads: 4\n",
      "2023-04-03 19:14:50,931 - INFO     | models.gpt4all | Loading model...\n",
      "llama_model_load: ggml ctx size = 4529.35 MB\n",
      "llama_model_load: memory_size =   512.00 MB, n_mem = 16384\n",
      "llama_model_load: loading model part 1/1 from '/Users/lorenzo/.nomic/gpt4all-lora-quantized.bin'\n",
      "llama_model_load: .................................... done\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n"
     ]
    }
   ],
   "source": [
    "gpt4all = GPT4ALL(ctx_size=512, n_predict=256)\n",
    "gpt4all.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15780b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 19:14:55,134 - INFO     | __main__   | Starting GPT4All generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada1710247784e6888bbbc410cf806dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 19:15:06,590 - INFO     | models.gpt4all | Loading model...\n",
      "llama_model_load: ggml ctx size = 4529.35 MB\n",
      "llama_model_load: memory_size =   512.00 MB, n_mem = 16384\n",
      "llama_model_load: loading model part 1/1 from '/Users/lorenzo/.nomic/gpt4all-lora-quantized.bin'\n",
      "llama_model_load: .................................... done\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tweet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(output_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m      4\u001b[0m     gpt4all\u001b[38;5;241m.\u001b[39minit_prompt(PROMPT_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(tweet\u001b[38;5;241m=\u001b[39mtweet))\n\u001b[0;32m----> 5\u001b[0m     gpt4all_out \u001b[38;5;241m=\u001b[39m \u001b[43mgpt4all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     gpt4all_outs\u001b[38;5;241m.\u001b[39mappend(gpt4all_out)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/repos/msc-thesis-llm-clustering/models/gpt4all.py:35\u001b[0m, in \u001b[0;36mGPT4ALL.generate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning Text Generation\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     37\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Returning empty string\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/nomic/gpt4all/gpt4all.py:187\u001b[0m, in \u001b[0;36mGPT4All.prompt\u001b[0;34m(self, prompt, write_to_stdout)\u001b[0m\n\u001b[1;32m    185\u001b[0m bot\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m bot\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m--> 187\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_to_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_to_stdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continuous_session:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/nomic/gpt4all/gpt4all.py:154\u001b[0m, in \u001b[0;36mGPT4All._parse_to_prompt\u001b[0;34m(self, write_to_stdout)\u001b[0m\n\u001b[1;32m    152\u001b[0m bot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbot\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     point \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m         character \u001b[38;5;241m=\u001b[39m point\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger.info('Starting GPT4All generation...')\n",
    "gpt4all_outs = []\n",
    "for i, tweet in enumerate(tqdm(output_tweets['full_text'])):\n",
    "    gpt4all.init_prompt(PROMPT_TEMPLATE.format(tweet=tweet))\n",
    "    gpt4all_out = gpt4all.generate()\n",
    "    gpt4all_outs.append(gpt4all_out)\n",
    "    if i % 5 == 0:\n",
    "        gpt4all.load_model()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1edf1bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. The tweet was posted by an individual who has never interacted with any brand before but seems interested in their products/services based on what they have said about them online, and is also active on social media platforms such as Instagram & Twitter. Based upon this information provided, would you classify the given tweeit as a potential lead?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4all_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9755655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
