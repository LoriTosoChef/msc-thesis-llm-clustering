{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 08:16:26,883 - INFO     | config     | Loading environment variables\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models.generation import Model\n",
    "\n",
    "from config import HUGGINGFACE_TOKEN, OPENAI_API_KEY, LLAMA_7B_PATH, GPT4ALL_PATH\n",
    "\n",
    "from helpers.data_helpers import save_to_parquet\n",
    "from helpers.generation_helpers import generation_loop\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Flags\n",
    "SLOW_COOLDOWN = 120\n",
    "FAST_COOLDOWN = 10\n",
    "\n",
    "N_TWEETS = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 20:08:16,012 - INFO     | __main__   | 21711 tweets in generation set.\n",
      "2023-06-25 20:08:16,013 - INFO     | __main__   | Generating from 1000 tweets.\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_parquet('data/eval_tweets_202342.parquet')\n",
    "logger.info(f'{len(tweets)} tweets in generation set.')\n",
    "logger.info(f'Generating from {N_TWEETS} tweets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tweets\n",
    "output_tweets = tweets.iloc[:N_TWEETS].copy()\n",
    "output_tweets.drop(columns=['created_at', 'entities'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1642025557511532545</td>\n",
       "      <td>the white paws, the cute collar, the tongue, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1642522139130486787</td>\n",
       "      <td>city boy. see those sneakers abeg. the way he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1642165846842966016</td>\n",
       "      <td>put it on a flame/heat safe surface and burn/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1642505628181569538</td>\n",
       "      <td>7| acquisitions: nike has made several acquisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1642367629020266496</td>\n",
       "      <td>before: bilas air hangat aftercare: moisturizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1641256201404186624</td>\n",
       "      <td>flowers themselves are $15-$25 and above.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1642436855739154436</td>\n",
       "      <td>i don't know much about shea butter but i kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1642545614427914244</td>\n",
       "      <td>thank you maybelline #maybellinexbini #bini_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1642342056637915141</td>\n",
       "      <td>super excited to finally get this out. the mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1640499261006544897</td>\n",
       "      <td>using our data, we see a very small increase ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                          full_text\n",
       "0    1642025557511532545  the white paws, the cute collar, the tongue, t...\n",
       "1    1642522139130486787  city boy. see those sneakers abeg. the way he ...\n",
       "2    1642165846842966016   put it on a flame/heat safe surface and burn/...\n",
       "3    1642505628181569538  7| acquisitions: nike has made several acquisi...\n",
       "4    1642367629020266496    before: bilas air hangat aftercare: moisturizer\n",
       "..                   ...                                                ...\n",
       "995  1641256201404186624         flowers themselves are $15-$25 and above. \n",
       "996  1642436855739154436   i don't know much about shea butter but i kno...\n",
       "997  1642545614427914244   thank you maybelline #maybellinexbini #bini_f...\n",
       "998  1642342056637915141  super excited to finally get this out. the mor...\n",
       "999  1640499261006544897   using our data, we see a very small increase ...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Answer the question based on the context below. \\\n",
    "    Context: You are a marketing and customer relationship management assistant, \\\n",
    "    your task is to classify a given tweet as either a \\\n",
    "    potential lead or not. Provide your detailed analysis of the following tweet \\\n",
    "    as a potential lead in the context of marketing and customer relationship management. \n",
    "    Answer with less than 100 words. \\\n",
    "    Tweet: \"{tweet}\" \\\n",
    "    Question: Is the above tweet a potential lead? Yes or No? Why?.\n",
    "    Answer: \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 14:26:19,836 - INFO     | models.generation | \n",
      "Initializing OPENAI model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 500\n",
      "2023-06-17 14:26:19,847 - INFO     | helpers.generation_helpers | Starting OPENAI generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2244776cec4644cdac667c9fae488089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 14:29:23,784 - INFO     | openai     | error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 14:29:23,785 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-06-17 14:38:15,013 - INFO     | helpers.generation_helpers | Step: 50 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 14:38:15,018 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 14:44:35,263 - INFO     | openai     | error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4fba65ae2c365291e58dfeb7d454a372 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 14:44:35,265 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4fba65ae2c365291e58dfeb7d454a372 in your message.).\n",
      "2023-06-17 14:49:56,441 - INFO     | helpers.generation_helpers | Step: 100 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 14:49:56,450 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 14:51:01,386 - INFO     | openai     | error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 14:51:01,386 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-06-17 14:51:35,693 - INFO     | openai     | error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f6744f7a30a14a926bda686671379a7d in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 14:51:35,695 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f6744f7a30a14a926bda686671379a7d in your message.).\n",
      "2023-06-17 15:01:52,529 - INFO     | helpers.generation_helpers | Step: 150 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 15:01:52,535 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 15:12:10,633 - INFO     | openai     | error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7f43cfffb48f25a26355c88cad4186d4 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 15:12:10,634 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7f43cfffb48f25a26355c88cad4186d4 in your message.).\n",
      "2023-06-17 15:13:44,355 - INFO     | helpers.generation_helpers | Step: 200 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 15:13:44,411 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 15:18:26,433 - INFO     | openai     | error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c9110b38761a27904ab6902a7944f15e in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 15:18:26,435 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c9110b38761a27904ab6902a7944f15e in your message.).\n",
      "2023-06-17 15:25:29,418 - INFO     | helpers.generation_helpers | Step: 250 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 15:25:29,425 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 15:31:48,636 - INFO     | openai     | error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d98757f7a40ff6f0ba649848eb238356 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 15:31:48,636 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d98757f7a40ff6f0ba649848eb238356 in your message.).\n",
      "2023-06-17 15:38:06,587 - INFO     | helpers.generation_helpers | Step: 300 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 15:38:06,595 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 15:42:32,163 - INFO     | openai     | error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 15:42:32,164 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-06-17 15:50:16,876 - INFO     | helpers.generation_helpers | Step: 350 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 15:50:16,895 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 16:03:29,701 - INFO     | helpers.generation_helpers | Step: 400 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 16:03:29,715 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 16:14:43,292 - INFO     | helpers.generation_helpers | Step: 450 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 16:14:43,313 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 16:16:31,557 - INFO     | openai     | error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a5a4acfb660c4211ec14b4f2cc9f3208 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 16:16:31,561 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a5a4acfb660c4211ec14b4f2cc9f3208 in your message.).\n",
      "2023-06-17 16:22:55,404 - INFO     | openai     | error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a26fe3f2445c2de0aa682cb3036550c1 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 16:22:55,406 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a26fe3f2445c2de0aa682cb3036550c1 in your message.).\n",
      "2023-06-17 16:27:30,937 - INFO     | helpers.generation_helpers | Step: 500 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 16:27:30,952 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 16:29:54,852 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n",
      "2023-06-17 16:32:19,643 - INFO     | openai     | error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7f555074e2230e74faa8674d1e6c60b9 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 16:32:19,645 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7f555074e2230e74faa8674d1e6c60b9 in your message.).\n",
      "2023-06-17 16:41:22,852 - INFO     | helpers.generation_helpers | Step: 550 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 16:41:22,869 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 16:42:48,162 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n",
      "2023-06-17 16:48:03,475 - INFO     | openai     | error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 20ea0288d9e8cd6841e169446e51d75b in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 16:48:03,478 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 20ea0288d9e8cd6841e169446e51d75b in your message.).\n",
      "2023-06-17 16:50:50,789 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n",
      "2023-06-17 16:54:59,125 - INFO     | openai     | error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e5258b99868b4161b4a87fef30405a97 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 16:54:59,127 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e5258b99868b4161b4a87fef30405a97 in your message.).\n",
      "2023-06-17 16:55:57,167 - INFO     | helpers.generation_helpers | Step: 600 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 16:55:57,182 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 17:06:51,819 - INFO     | helpers.generation_helpers | Step: 650 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 17:06:51,841 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 17:08:54,822 - INFO     | openai     | error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 050a29fa2aa7c2cc0598778cc2c9c030 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 17:08:54,825 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 050a29fa2aa7c2cc0598778cc2c9c030 in your message.).\n",
      "2023-06-17 17:18:18,520 - INFO     | helpers.generation_helpers | Step: 700 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 17:18:18,534 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 17:29:20,959 - INFO     | helpers.generation_helpers | Step: 750 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 17:29:20,980 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 17:33:12,953 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n",
      "2023-06-17 17:35:34,020 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n",
      "2023-06-17 17:36:43,655 - INFO     | openai     | error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-17 17:36:43,664 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-06-17 17:42:39,925 - INFO     | helpers.generation_helpers | Step: 800 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 17:42:39,951 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 17:53:54,104 - INFO     | helpers.generation_helpers | Step: 850 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 17:53:54,121 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 18:06:01,344 - INFO     | helpers.generation_helpers | Step: 900 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 18:06:01,369 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 18:13:13,706 - WARNING  | /Users/lorenzo/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chat_models/openai.py | Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n",
      "2023-06-17 18:18:53,141 - INFO     | helpers.generation_helpers | Step: 950 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 18:18:53,168 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 18:30:16,842 - INFO     | helpers.generation_helpers | Step: 1000 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 18:30:16,872 - INFO     | helpers.data_helpers | 200T.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI Model\n",
    "openai = Model(model_name='openai',\n",
    "               openai_api=OPENAI_API_KEY,\n",
    "               openai_model='gpt-3.5-turbo',\n",
    "               max_tokens=500)\n",
    "\n",
    "# injecting prompts\n",
    "openai.init_prompt(template=PROMPT_TEMPLATE,\n",
    "                   input_vars=['tweet'])\n",
    "\n",
    "# generation\n",
    "output_tweets = generation_loop(model=openai,\n",
    "                           model_col='gpt-3.5-turbo',\n",
    "                           n=N_TWEETS,\n",
    "                           tweets=output_tweets,\n",
    "                           fast_cool=FAST_COOLDOWN,\n",
    "                           slow_cool=SLOW_COOLDOWN,\n",
    "                           out_dir='outputs',\n",
    "                           out_name='200T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 18:30:16,907 - INFO     | models.generation | \n",
      "Initializing ALPACA-770M model  - Temp: 1e-10 - Context window: 2048 - Max tokens: 500\n",
      "2023-06-17 18:30:18,304 - INFO     | helpers.generation_helpers | Starting ALPACA-770M generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fd6128527b4a08b521aea000b036e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 18:39:06,487 - INFO     | helpers.generation_helpers | Step: 50 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 18:39:06,507 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 18:47:53,303 - INFO     | helpers.generation_helpers | Step: 100 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 18:47:53,328 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 18:58:54,265 - INFO     | helpers.generation_helpers | Step: 150 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 18:58:54,289 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 19:13:12,072 - INFO     | helpers.generation_helpers | Step: 200 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 19:13:12,089 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 19:27:57,822 - INFO     | helpers.generation_helpers | Step: 250 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 19:27:57,846 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 19:42:13,522 - INFO     | helpers.generation_helpers | Step: 300 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 19:42:13,532 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 19:56:57,329 - INFO     | helpers.generation_helpers | Step: 350 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 19:56:57,353 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 20:11:18,386 - INFO     | helpers.generation_helpers | Step: 400 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 20:11:18,409 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 20:25:43,250 - INFO     | helpers.generation_helpers | Step: 450 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 20:25:43,272 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 20:40:34,559 - INFO     | helpers.generation_helpers | Step: 500 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 20:40:34,581 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 20:54:15,418 - INFO     | models.generation | RATE LIMIT! Waiting for 5 minutes\n",
      "2023-06-17 20:54:15,421 - WARNING  | models.generation | Error raised by inference API: Internal Server Error\n",
      "2023-06-17 21:00:12,873 - INFO     | helpers.generation_helpers | Step: 550 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 21:00:12,895 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 21:14:47,165 - INFO     | helpers.generation_helpers | Step: 600 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 21:14:47,226 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 21:28:50,336 - INFO     | helpers.generation_helpers | Step: 650 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 21:28:50,354 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 21:43:14,305 - INFO     | helpers.generation_helpers | Step: 700 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 21:43:14,331 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 21:58:06,652 - INFO     | helpers.generation_helpers | Step: 750 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 21:58:06,679 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 22:11:50,825 - INFO     | helpers.generation_helpers | Step: 800 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 22:11:50,850 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 22:25:59,605 - INFO     | helpers.generation_helpers | Step: 850 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 22:25:59,632 - INFO     | helpers.data_helpers | 200T.parquet saved.\n",
      "2023-06-17 22:39:52,501 - INFO     | helpers.generation_helpers | Step: 900 - Saving checkpoint and cooldown for 2.0m...\n",
      "2023-06-17 22:39:52,528 - INFO     | helpers.data_helpers | 200T.parquet saved.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m alpaca\u001b[39m.\u001b[39minit_prompt(template\u001b[39m=\u001b[39mPROMPT_TEMPLATE,\n\u001b[1;32m      9\u001b[0m                    input_vars\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[39m# generation\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m output_tweets \u001b[39m=\u001b[39m generation_loop(model\u001b[39m=\u001b[39;49malpaca,\n\u001b[1;32m     13\u001b[0m                                 model_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39malpaca\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m                                 n\u001b[39m=\u001b[39;49mN_TWEETS,\n\u001b[1;32m     15\u001b[0m                                 tweets\u001b[39m=\u001b[39;49moutput_tweets,\n\u001b[1;32m     16\u001b[0m                                 fast_cool\u001b[39m=\u001b[39;49mFAST_COOLDOWN,\n\u001b[1;32m     17\u001b[0m                                 slow_cool\u001b[39m=\u001b[39;49mSLOW_COOLDOWN,\n\u001b[1;32m     18\u001b[0m                                 out_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moutputs\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m                                 out_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m200T\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/repos/msc-thesis-llm-clustering/helpers/generation_helpers.py:27\u001b[0m, in \u001b[0;36mgeneration_loop\u001b[0;34m(model, model_col, n, tweets, fast_cool, slow_cool, out_dir, out_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m outs \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n)]\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m i, tweet \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(tweets[\u001b[39m'\u001b[39m\u001b[39mfull_text\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[1;32m     26\u001b[0m     \u001b[39m# generate\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     llm, out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inject_obj\u001b[39m=\u001b[39;49mtweet)\n\u001b[1;32m     28\u001b[0m     \u001b[39m# insert output\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     outs[i] \u001b[39m=\u001b[39m out\n",
      "File \u001b[0;32m~/Documents/repos/msc-thesis-llm-clustering/models/generation.py:89\u001b[0m, in \u001b[0;36mModel.generate\u001b[0;34m(self, inject_obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRunning Text Generation\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m     out \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mrun(inject_obj)\n\u001b[1;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     91\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRATE LIMIT! Waiting for 5 minutes\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/llm.py:57\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply([inputs])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/llm.py:118\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[1;32m    117\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[1;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/chains/llm.py:62\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(prompts, stop)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:107\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m, prompts: List[PromptValue], stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    106\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/base.py:324\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    322\u001b[0m generations \u001b[39m=\u001b[39m []\n\u001b[1;32m    323\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m--> 324\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    325\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    326\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/langchain/llms/huggingface_hub.py:103\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to HuggingFace Hub's inference endpoint.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m        response = hf(\"Tell me a joke.\")\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m _model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m--> 103\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient(inputs\u001b[39m=\u001b[39;49mprompt, params\u001b[39m=\u001b[39;49m_model_kwargs)\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m response:\n\u001b[1;32m    105\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError raised by inference API: \u001b[39m\u001b[39m{\u001b[39;00mresponse[\u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/huggingface_hub/inference_api.py:182\u001b[0m, in \u001b[0;36mInferenceApi.__call__\u001b[0;34m(self, inputs, params, data, raw_response)\u001b[0m\n\u001b[1;32m    179\u001b[0m     payload[\u001b[39m\"\u001b[39m\u001b[39mparameters\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m params\n\u001b[1;32m    181\u001b[0m \u001b[39m# Make API call\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mpost(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_url, headers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders, json\u001b[39m=\u001b[39;49mpayload, data\u001b[39m=\u001b[39;49mdata)\n\u001b[1;32m    184\u001b[0m \u001b[39m# Let the user handle the response\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m raw_response:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/thesis/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Alpaca Model from Stanford, based on LLAMA\n",
    "alpaca = Model(model_name='alpaca-770M',\n",
    "               hf_api=HUGGINGFACE_TOKEN,\n",
    "               hf_repo='declare-lab/flan-alpaca-large',\n",
    "               max_tokens=500)\n",
    "\n",
    "# injecting prompt\n",
    "alpaca.init_prompt(template=PROMPT_TEMPLATE,\n",
    "                   input_vars=['tweet'])\n",
    "\n",
    "# generation\n",
    "output_tweets = generation_loop(model=alpaca,\n",
    "                                model_col='alpaca',\n",
    "                                n=N_TWEETS,\n",
    "                                tweets=output_tweets,\n",
    "                                fast_cool=FAST_COOLDOWN,\n",
    "                                slow_cool=SLOW_COOLDOWN,\n",
    "                                out_dir='outputs',\n",
    "                                out_name='200T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base LLAMA Model\n",
    "llama = Model(model_name='llama',\n",
    "              n_threads=6,\n",
    "              local_model_path=LLAMA_7B_PATH,\n",
    "              max_tokens=500)\n",
    "\n",
    "# injecting prompt\n",
    "llama.init_prompt(template=PROMPT_TEMPLATE,\n",
    "                  input_vars=['tweet'])\n",
    "\n",
    "# generation\n",
    "output_tweets = generation_loop(model=llama,\n",
    "                                model_col='llama',\n",
    "                                n=N_TWEETS,\n",
    "                                tweets=output_tweets,\n",
    "                                fast_cool=FAST_COOLDOWN,\n",
    "                                slow_cool=SLOW_COOLDOWN,\n",
    "                                out_dir='outputs',\n",
    "                                out_name='200T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT4ALL, Fine-Tuned with OpenAI's model Q&As\n",
    "gpt4all = Model(model_name='gpt4all',\n",
    "                n_threads=6,\n",
    "                local_model_path=GPT4ALL_PATH,\n",
    "                max_tokens=500)\n",
    "\n",
    "# injecting prompt\n",
    "gpt4all.init_prompt(template=PROMPT_TEMPLATE,\n",
    "                    input_vars=['tweet'])\n",
    "\n",
    "# generation\n",
    "output_tweets = generation_loop(model=gpt4all,\n",
    "                                model_col='gpt4all',\n",
    "                                n=N_TWEETS,\n",
    "                                tweets=output_tweets,\n",
    "                                fast_cool=FAST_COOLDOWN,\n",
    "                                slow_cool=SLOW_COOLDOWN,\n",
    "                                out_dir='outputs',\n",
    "                                out_name='200T')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gpt-3.5-turbo', 'alpaca'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tweets and generations\n",
    "df = pd.read_parquet('outputs/1000T_2023617.parquet')\n",
    "\n",
    "# Select model columns\n",
    "model_cols = df.columns[2:]\n",
    "model_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 08:18:22,990 - INFO     | models.embeddings | Initializing DISTIL-ROBERTA for Sentence Embeddings\n",
      "2023-06-26 08:18:22,991 - INFO     | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: all-distilroberta-v1\n",
      "2023-06-26 08:18:23,864 - INFO     | sentence_transformers.SentenceTransformer | Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from models.embeddings import SentenceEmbeddings\n",
    "\n",
    "# Using distil-roberta for the sentence embeddings\n",
    "distilrberta = SentenceEmbeddings(name='distil-roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 08:18:27,090 - INFO     | __main__   | Parsing model: gpt-3.5-turbo\n",
      "2023-06-26 08:18:27,090 - INFO     | models.embeddings | DISTIL-ROBERTA - Generating sentence embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d11492a9154d9e91ec870dab1a393e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 08:18:39,467 - INFO     | __main__   | Parsing model: alpaca\n",
      "2023-06-26 08:18:39,468 - INFO     | models.embeddings | DISTIL-ROBERTA - Generating sentence embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a8a8e1a28b4c97991b0d0a298840b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generating embeddings\n",
    "distil_dict = {}\n",
    "for col in model_cols:\n",
    "    logger.info(f'Parsing model: {col}')\n",
    "    distil_dict[col] = distilrberta.generate_embeddings(input_texts=df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <th>alpaca</th>\n",
       "      <th>gpt-3.5-turbo_embeddings</th>\n",
       "      <th>alpaca_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1642025557511532545</td>\n",
       "      <td>the white paws, the cute collar, the tongue, t...</td>\n",
       "      <td>No, the above tweet is not a potential lead. T...</td>\n",
       "      <td>Yes, the tweet is a potential lead because it ...</td>\n",
       "      <td>[-0.060498107224702835, -0.06838816404342651, ...</td>\n",
       "      <td>[-0.02545035257935524, -0.11641273647546768, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1642522139130486787</td>\n",
       "      <td>city boy. see those sneakers abeg. the way he ...</td>\n",
       "      <td>No, the above tweet is not a potential lead. T...</td>\n",
       "      <td>Yes, the tweet is a potential lead because it ...</td>\n",
       "      <td>[-0.0260239876806736, -0.08790598064661026, -0...</td>\n",
       "      <td>[-0.02217881567776203, -0.10774783790111542, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1642165846842966016</td>\n",
       "      <td>put it on a flame/heat safe surface and burn/...</td>\n",
       "      <td>No, the above tweet is not a potential lead. I...</td>\n",
       "      <td>Yes, this tweet is a potential lead because it...</td>\n",
       "      <td>[-0.028097709640860558, -0.09122901409864426, ...</td>\n",
       "      <td>[-0.032287366688251495, -0.08707711845636368, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1642505628181569538</td>\n",
       "      <td>7| acquisitions: nike has made several acquisi...</td>\n",
       "      <td>No, the above tweet is not a potential lead. W...</td>\n",
       "      <td>Yes, the tweet is a potential lead because it ...</td>\n",
       "      <td>[-0.012716451659798622, -0.10650305449962616, ...</td>\n",
       "      <td>[-0.007112530060112476, -0.09131787717342377, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1642367629020266496</td>\n",
       "      <td>before: bilas air hangat aftercare: moisturizer</td>\n",
       "      <td>No, the above tweet is not a potential lead in...</td>\n",
       "      <td>Yes, this tweet is a potential lead because it...</td>\n",
       "      <td>[-0.029857849702239037, -0.1002630740404129, -...</td>\n",
       "      <td>[-0.02748551405966282, -0.11469537019729614, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                          full_text  \\\n",
       "0  1642025557511532545  the white paws, the cute collar, the tongue, t...   \n",
       "1  1642522139130486787  city boy. see those sneakers abeg. the way he ...   \n",
       "2  1642165846842966016   put it on a flame/heat safe surface and burn/...   \n",
       "3  1642505628181569538  7| acquisitions: nike has made several acquisi...   \n",
       "4  1642367629020266496    before: bilas air hangat aftercare: moisturizer   \n",
       "\n",
       "                                       gpt-3.5-turbo  \\\n",
       "0  No, the above tweet is not a potential lead. T...   \n",
       "1  No, the above tweet is not a potential lead. T...   \n",
       "2  No, the above tweet is not a potential lead. I...   \n",
       "3  No, the above tweet is not a potential lead. W...   \n",
       "4  No, the above tweet is not a potential lead in...   \n",
       "\n",
       "                                              alpaca  \\\n",
       "0  Yes, the tweet is a potential lead because it ...   \n",
       "1  Yes, the tweet is a potential lead because it ...   \n",
       "2  Yes, this tweet is a potential lead because it...   \n",
       "3  Yes, the tweet is a potential lead because it ...   \n",
       "4  Yes, this tweet is a potential lead because it...   \n",
       "\n",
       "                            gpt-3.5-turbo_embeddings  \\\n",
       "0  [-0.060498107224702835, -0.06838816404342651, ...   \n",
       "1  [-0.0260239876806736, -0.08790598064661026, -0...   \n",
       "2  [-0.028097709640860558, -0.09122901409864426, ...   \n",
       "3  [-0.012716451659798622, -0.10650305449962616, ...   \n",
       "4  [-0.029857849702239037, -0.1002630740404129, -...   \n",
       "\n",
       "                                   alpaca_embeddings  \n",
       "0  [-0.02545035257935524, -0.11641273647546768, -...  \n",
       "1  [-0.02217881567776203, -0.10774783790111542, -...  \n",
       "2  [-0.032287366688251495, -0.08707711845636368, ...  \n",
       "3  [-0.007112530060112476, -0.09131787717342377, ...  \n",
       "4  [-0.02748551405966282, -0.11469537019729614, -...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert embeddings in dataframe\n",
    "for col in model_cols:\n",
    "    embeddings_col = col + '_embeddings'\n",
    "    temp = pd.DataFrame({embeddings_col: distil_dict[col].tolist()})\n",
    "    df[embeddings_col] = temp[embeddings_col].copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 08:33:14,478 - INFO     | helpers.data_helpers | full_data.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "# save full dataframe with embeddings and embeddings also separately\n",
    "save_to_parquet(data_dir='.', df=df, name='full_data')\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(f'embeddings/1000T_embeddings_2023617.pkl', 'wb') as f:\n",
    "    pickle.dump(distil_dict, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
